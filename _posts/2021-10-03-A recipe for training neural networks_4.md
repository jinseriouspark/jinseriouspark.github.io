---
layout: post
title: "A recipe for training neural networks 신경망 학습 가이드(3)"
author: "Seriouscoding"
---

오역이 있을시 말씀주시면 수정하겠습니다.

**원제:  recipe for training neural networks** [post url](http://karpathy.github.io/2019/04/25/recipe/)

**저자: Andrej Karpathy**


______________________________________________________________

3. Overfit 과적합

    이번 단계에서, 우리는 데이터셋에 대한 좋은 이해가 필요하며, 전체 학습 + 평가 파이프라인을 다룰 것입니다. 우리가 신뢰할 수 있는 metric 을 (재현 가능하게) 계산할 수 있는 어떤 모델이든 말입니다. 우리는 인풋-독립적인 베이스라인에 대한 성능, 몇 가지 바보같은 베이스라인의 성능 (이것보단 잘해야 함)을 가지고 있으며, 사람 평가(실제로 도달하고 싶은 목표) 에 대한 대략적인 감각도 있습니다. 이제 좋은 모델을 반복하기 위한 단계를 모두 갖췄습니다.

    이 좋은 모델을 찾기 위해 기꺼이 할 이 접근은 두 단계로 구성되어있습니다: 먼저, 모델이 오버핏(예: 학습 오차에 집중) 될 수 있도록 충분한 크기로 만들어 두세요, 그리고 적절하게 정규화를 진행하세요( 검증 오차를 개선하기 위해 학습 오차를 포기할 수 있을 정도로). 제가 두 단계를 좋아하는 이유는, 우리는 특정 이슈, 버그, 잘못된 설정 오류들로 인해 낮은 에러율 때문에 어떠한 모델로도 낮은 에러율에 도달할 수 없기 때문입니다.

    몇가지 팁과 트릭이 있습니다

    - picking the model 모델을 선택하세요
    
        좋은 학습 오차에 도달하기 위해 여러분의 데이터에 맞는 적절한 설계 구조를 선택하기를 원할 것입니다. #1의 조언에서 말한 것과 같이, 영웅이 되려고 하지 마십시오(Don't be a hero). 저는 수 많은 사람들이, 그들에게 합리적이라고 보이는 다양하게 특이한 아키텍쳐 속의 툴박스에서 신경망 레고 블록을 가득 쌓는 창의성을 보이거나 그것에 미칠정도의 열정을 가진 사람들을 많이 보았습니다. 여러분의 프로젝트 초기 단계에서 그러한 유혹에 강하게 저항하십시오. 저는 언제나 사람들에게 가장 관련성 높은 논문을 찾으라고 조언하고, 좋은 퍼포먼스를 기록한 간단한 아키텍쳐를 그대로 복사 붙여넣기 하라고 이야기를 합니다. 예를들어, 여러분이 이미지를 분석할 때, 영웅이 되지 않고 ResNet-50 데이터를 단순 복사 붙여넣기하여 처음 실행한다고 생각해보세요. 여러분들은 다음에 더 개인화 할 수 있으며 결국 이것을 해결할 수 있을 것입니다. 
    
    - adam is safe 아담은 안전합니다
    
        베이스라인을 셋팅하는 초기 단계에서, 저는 학습률 3e-4의 Adam을 사용하는 것을 좋아합니다. 제 경험에 비추어 보면, Adam 은 나쁜 학습률을 포함한 수많은 하이퍼 파라미터의 영향력을 없에버리곤 합니다(forgive to hyperparameters). ConvNets에 있어서, 잘 튜닝된 SGD가 거의 언제나 Adam보다 조금 더 나은 성능을 냅니다. 그러나 최적화된 학습률 범위는 더욱 미세하고, 문제 지향적입니다. (기억하세요: 만약 여러분이 RNN을 사용하고, 관련된 시퀀스 모델을 사용하고자 한다면, Adam을 먼저 사용하는 것이 일반적입니다. 여러분의 프로젝트 초기 단계에서, 다시말하자면, 영웅이 되려고 하지 마시고 대부분의 논문이 한 것을 따르세요)
    
    - complexify only one at a time 한번에 하나만 복잡하게 하기
    
        여러분의 분류기에 다수의 신호를 넣는다면, 저는 여러분이 하나 하나씩 넣기를 바라며 매 순간마다 여러분이 예상한 것과 같은 수준의 퍼포먼스를 올렸는지 확인하기를 바랍니다. 여러분의 모델에 부엌 싱크대에 던져놓듯이 시작부터 쏟아붙지 마세요. 복잡도를 높히는 다른 방법이 있습니다. - 여러분이 작은 이미지를 먼저 넣어보려는 시도를 한 뒤에 더 큰 이미지를 나중에 넣는 등 말입니다.
 
    - do not trust learning rate decay defaults 학습률 decay defaults 를 신뢰하지 마세요
 
        만약 여러분이 재 목적 코드를 다른 도메인에서 만든다면, 언제나 learning rate decay에 대해 신중해야 합니다. 다양한 문제들에 대한 여러 decay 스케쥴을 사용하기를 원할 뿐더러, 심지어 - 더 안좋게도- 전형적인 구현 속에서 이 스케쥴은 현재 에포크 숫자를 기반으로 하는데, 이는 여러분의 데이터셋 사이즈에 기반하여 매우 널리 간단하게 다양화 할 수 있습니다. 예를들어, ImageNet은 decay를 에포크 30에 10으로 둘 수 있습니다. 만약 ImageNet을 학습하지 않는다면, 여러분은 거의 당연하게도 이를 원하지 않을 것입니다. 만약 여러분의 코드에 심혈을 기울이지 않는다면, 이는 비밀스럽게 여러분의 학습률을 너무 빠르게 0으로 만들것이고, 여러분의 모델은 수렴하지 않게 될 것입니다. 제 업무에서, 저는 언제나 학습률 decay를 전체적으로 사용하지 못하게 두었고,(저는 상수 LR을 사용했습니다) 맨 마지막에서야 튜닝하였습니다.


 4. Regularize 정규화

    그리고 만약 여러분의 네트워크가, 데이터 안에서 여러분이 발견한 몇 예측값을 제공하는 구역에 있습니다. 이제 그 값을 정규화 하여, 학습 시 정확도를 좀 포기하면서 검증 정확도를 높혀야 할 때가 왔습니다. 몇가지 팁과 전략을 드립니다:

    - get more data 더 많은 데이터를 모으세요
    
    먼저, 그리고 가장 좋고 선호되는, 실전적인 모델 정규화 방법은 실제 학습 데이터를 더 추가하는 것입니다. 많은 엔지니어링 사이클에서, 아주 작은 데이터셋의 주스 즙을 짜내는 듯한 실수가 발생하는 것은 일반적입니다. 그 대신에, 여러분은 더 많은 데이터를 모아야 합니다. 더 많은 데이터를 추가하는 작업이 잘 정리된 신경망을 거의 무한정한 퍼포먼스로 개선할 수 있다고 보증합니다. 그렇지 않다면, 앙상블을 시도하는 것인데 (가능하다면) 이는 상위 5개 모델정도면 됩니다.
    
    -   data augment 데이터 증강
    
        그 다음으로 실제 데이터에 가장 좋은 처리는 half-fake 데이터를 만드는 것입니다. - 더 공격적인 데이터 증강을 하시면 됩니다. 
    
    - creative augmentation 
        만약 half-fake data 가 더이상 역할을 하지 않는다면, fake data 에도 또 다른 처리가 필요합니다. 사람들은 데이터셋을 늘리기 위한 창의적인 방법을 찾고있습니다. 예를 들어, domain randomization 도메인 랜덤화, use of simulation 시뮬레이션 사용, clever hybrids 똑똑한 혼합 등, 잠재적으로 시뮬레이팅된 데이터를 삽입하는 것도 있습니다. 심지어 GAN에도 말입니다.
    
    - pretrain 선행학습
    
        가능하다면, 선행 학습된 네트워크를 사는 것은 거의 나쁜 영향을 끼치지 않을 것입니다. 심지어 여러분이 충분한 데이터를 가지고 있어도 말입니다. 
    
    - stick with supervised learning 지도 학습을 고수하십시오
    
        비지도 선행학습에 대해 너무 많은 흥미를 느끼지 마세요. 2008년의 블로그 포스트가 여러분에게 이야기 하는 것과 달리, 제가 아는 한, 그 어떤 버전도 현대 컴퓨터 비전에서 강력한 결과를 보고하지 않았습니다. (비록 NLP는 요즘 BERT및 관련 모델들과 꽤 잘지내는 것 같습니다. 테스트의 의도적인 특성 및 더 높은 신호 대 잡음 비율에 있어서 말입니다.
    
    - smaller input dimensionality 작은 인풋 차원에서 시작하십시오
    
        가짜 신호를 포함할 수 있는 피쳐를 제거하세요. 어떠한 가짜 인풋값을 추가하는 것은 데이터셋이 굉장히 작을 때 과적합을 발생할 수 있는 또 다른 기회가 됩니다. 이와 비슷하게, 만약 낮은 레벨의 디테일이 더이상 중요하지 않을 경우, 더 작은 이미지를 입력하세요.
    
    - smaller model size 모델 사이즈를 더 작게 만드세요
    
        많은 경우, 여러분들은 도메인 지식 제약을 네트워크에 사용하여 그 사이즈를 줄일 것입니다. 예를들어, 완전히 연결된 입력층을 이미지넷의 기본 가장 위에 사용하는 것이 유행이었지만, 이후에는 단순한 평균 풀링으로 대체되어 프로세스에서 수 많은 파라미터들이 제거되었습니다. 
    
    - decrease the batch size 배치 크기를 줄이세요
    
        배치 norm 내부의 정규화로 인해 배치 크기가 작을 수록 정규화가 더 강해집니다. 이는 배치 경험적인 mean, std (평균, 표준편차) 가 전체 평균, 표준편차와 유사한 수준이기 때문에, scale & offset 이 여러분의 배치를 더 많이 '흔들' 것입니다.