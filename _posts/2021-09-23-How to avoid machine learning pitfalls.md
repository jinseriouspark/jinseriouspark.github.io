---
layout: post
title: "어떻게 머신러닝 함정을 피할 수 있을까: Academic Researcher를 위한 가이드"
author: "Seriouscoding"
---

오역이 있을시 말씀주시면 수정하겠습니다.

**원제: How to avoid machine learning pitfalls: a guide for academic researchers** [arxiv 2108.02497](https://arxiv.org/abs/2108.02497)

**저자: Michale A. Lones**


______________________________________________________________

## 요약


이 문서는 머신러닝 기술을 사용할 때 발생하는 일반적인 실수 중의 몇가지를 이야기 하고 어떻게 하면 피할 수 있는지 관련된 간결한 가이드를 줍니다. 이 문서는 리서치 학생들을 위한 가이드로서 가장 중요하게 의도되었습니다. 그리고 엄격한 비교를 필요로 한다던가, 타당한 결론에 도달한다던가 하는, 학술적 연구 안에서 특히 염려되는 이슈들에 집중하였습니다. 머신러닝 과정의 5단계를 다룹니다: 모델을 생성하기 전에 무엇을 해야 하는지, 안정적인 모델을 어떻게 만드는지, 강건하게 모델 평가를 어떻게 할 수 있는지, 모델을 공정하게 비교하는 방법은 무엇인지, 그리고 결과를 어떻게 리포팅 하는지에 대해서 말입니다. 


### 목록
    1. 소개
    2. 모델을 생성하기 전에 해야 할 것
        1. 데이터를 충분히 이해하는 시간을 가지시오
        2. 여러분의 모든 데이터를 다 보려고 하지 마십시오
        3. 충분한 데이터를 확보하십시오
        4. 도메인 전문가와 이야기를 나누어 보십시오
        5. 많은 논문과 문서들을 찾아보십시오
        6. 여러분의 모델이 어떻게 이용될 지 생각해보십시오
    3. 신뢰할 수 있는 모델을 만드는 방법
        1. 학습 과정에서 테스트 데이터를 누출하지 마십시오
        2. 다양한 범위의 모델을 시도하십시오
        3. 부적절한 모델을 사용하지 마십시오
        4. 여러분 모델의 하이퍼파라미터를 최적화하십시오
        5. 하이퍼파라미터를 최적화 하고 피쳐를 선택할 때 주의하십시오
    4. 강건하게 모델을 평가하는 방법
        1. 적절한 테스트셋을 사용하십시오
        2. 검증 셋을 사용하십시오
        3. 여러번 모델 평가를 진행하십시오
        4. 여러분의 마지막 모델 인스턴스를 위해 따로 데이터를 저장해두십시오
        5. 불균형 데이터 셋에 accuracy를 적용하지 마십시오
    5. 공정하게 모델을 비교하는 방법
        1. 큰 수치가 더 나은 모델을 의미한다고 가정하지 마십시오
        2. 모델 비교 시 통계적 테스트를 사용하십시오
        3. 다수의 비교를 수정하십시오
        4. 커뮤니티 벤치마크도 틀릴 때가 있습니다(커뮤니티 벤치마크를 항상 신뢰하지 마십시오)
        5. 모델의 조합을 고려하십시오
    6. 여러분의 결과를 보고하는 방법
        1. 진실되게 행동하시오(투명하게 행동하시오)
        2. 다수의 방법으로 퍼포먼스를 정리해보시오
        3. 데이터를 넘어선 일반화를 하지 마시오
        4. 통계적 중요도를 보고할 때 주의하십시오
        5. 당신의 모델을 바라보십시오
    7. 최종 생각
    
    

## 1. 소개 

이 가이드는 학술 연구 과정 속에서 머신러닝을 사용할 떄 발생하는 몇 가지 실수를 피할 수 있도록 새로 들어온 사람들을 돕는 데에 목표를 두고 있습니다. 이 문서는 학술적으로 쓰여졌고, 아카데미아에서 ML 연구를 하는 과정 속에서, ML연구를 하는 학생들을 지도하는 과정 속에서 배울 수 있는 교훈에 집중하였습니다. 학생들을 우선적으로 목표로 두었고, ML 분야에 관련되어 일하는 다른 연구자도 고려하였습니다. (ML기술의 기초 지식만 가정하였습니다) 더 일반적인 ML 오디언스를 겨냥한 유사 가이드와 다르게, 이 문서는 아카데미아의 학술적인 고민들을 반영하고 있습니다: 철저하게 비교하는 것이 필요하거나, 논문 제출을 위한 모델 비교들이 그것입니다. 그러나, 대부분의 교훈들은 ML의 넓은 사용에 적용할 수 있고, 이는 필드에 들어온 누구에게나 할 수 있는 소개 가이드가 됩니다. 더 잘 읽힐 수 있도록, 이 가이드는 '해야 하는 것'과 '해서는 안되는 것' 스타일의 알려주기 방식으로 작성되었습니다. 좌절을 주기 위한 것은 아니며, 맨 뒤 reference 는 더 많은 읽을 거리를 제공해줍니다. 이것이 특정 아카데믹 주제들을 다루지 않기 때문에, 여러분들은 가능한 분야의 하위 특정 가이드로 생각해주기를 추천드립니다. 피드백은 언제나 환영이고요, 이 문서가 시공간을 너머 회자되기를 기대합니다. 이러한 이유때문에, 만약 여러분이 인용을 하신다면, arXiv 숫자를 같이 추가해주시기를 부탁드립니다. 



## 2. 모델을 생성하기 전에 해야 할 것


모델 학습과 평가에 급박하게 뛰어들고 싶은 것은 일반적입니다. 그러나 시간을 가지고 프로젝트의 목표에 대해 생각하는 것은 중요합니다. 또한 프로젝트의 목표들을 달성하는데에 사용되곤 하는 데이터를 완벽하게 이해하는 것도 중요합니다. 그리고 주장될 필요가 있는 데이터의 제한점에 대해서도 고려해야하며, 여러분의 분야에서 이미 진행된 것들이 무엇인지 이해하는 것도 필요합니다. 이러한 작업이 없다면, 여러분들은 제출하기 어려운 결과물로 끝을 맺을 수 있고, 그 의도된 목적에 부합하지 않는 모델을 만들어낼 수도 있습니다.


### 2.1 데이터를 충분히 이해하는 시간을 가지시오

결과적으로 여러분은 여러분 업무를 제출할 것입니다. 여러분의 데이터가 좋은 품질, 신뢰할만한 방법론을 사용하여 수집된, 믿을만한 데이터 소스(원천) 에서 나왔다면 더 쉬울 것입니다. 예를 들어, 인터넷 리소스에서 사용 데이터를 모았다면, 여러분들은 그 데이터의 출처를 밝혀야 할 것입니다. 이것들이 논문에 설명되어있나요? 만약 그렇다면, 그 논문을 살펴보아야 합니다; 꽤 명성이 자자한 결과인지 확인해야 합니다, 또한 작가가 데이터 제한에 대해 이야기를 했는지 아닌지 체크해야 합니다. 데이터가 수 많은 논문에 의해 사용되었기 때문에, 이 데이터는 좋은 품질일거야- 이런 식으로 가정하지 마십시오. 어떤 데이터들은 쉽게 얻을 수 있다는 것만으로도 사용되었습니다. 또 다른 널리 사용된 데이터는 큰 결함이 있는데도 사용된 것으로 알려져있습니다([Paullada et al., 2020] 에서 이 부분에 대한 논의를 진행합니다) 만약 여러분의 모델이 나쁜 데이터를 사용하여 학습되었다면, 여러분은 나쁜 모델을 생성한 것과 같습니다. 쓰레기를 넣으면 쓰레기가 나오는 과정과 동일합니다. 그렇기 때문에 여러분의 데이터가 타당성을 가지고 있는지 확인하는 작업부터 항상 시작하십시오. 탐색적 데이터 분석을 하십시오. 결측이 되거나 일관성이 없는 데이터인지 살펴보십시오. 이후에, 만약 여러분이 리뷰어에게 왜 저품질의 데이터를 사용하게 되었는지 설명하려고 할 때 보다, 모델이 학습하기 이전인, 지금 하는 것이 훨씬 쉽습니다. 

### 2.2 여러분의 모든 데이터를 다 보려고 하지마십시오

여러분의 데이터를 살펴보면, 국소 패턴과, 모델링에 가이드를 줄 수 있는 통찰을 얻을 수 있을 것입니다. 이는 데이터를 살펴보는 또 다른 장점입니다. 그러나, 여러분의 모델에 주입될, 테스트할 수 없는(untestable) 가정을 만들지 않는 것은 중요합니다. 이 "테스트 할 수 없음(untestable)"은 이 지점에서 꽤 중요합니다; 가설을 만드는 것 까지는 좋습니다, 그러나 모델의 학습에만 주입되어야 합니다. 테스트가 아니라요. 이러한 경우, 여러분은 테스트 데이터를 면밀하게 살펴봄으로써 초기 탐색적 분석 단계에 포함되지 않도록 주의해야 합니다. 그렇지 않으면 의식적이거나 무의식적으로, 테스트 할 수 없는 방법 하에서 여러분 모델의 일반화에 제한이 있다는 가정을 하게 될 것입니다. 테스트셋을 학습 과정에 넣는 것은 ML모델이 일반화에 실패하는 아주 공통적인 원인이면서 저 또한 몇번 반복했던 주제입니다. 이것들을 피하기 위해 학습 과정에 테스트 데이터를 누출하지 마십시오

### 2.3  충분한 데이터를 확보하십시오

충분한 데이터가 없다면, 일반화를 위한 모델 학습이 불가능할 것입니다. 이러한 경우를 다루는 것은 꽤 어려운데요, 여러분이 모델을 만들때까지 그 어떤 증거가 없기 때문입니다: 데이터셋의 노이즈 비율 신호에만 의존해야 합니다. 만약 신호가 강하다면, 여러분은 몇몇 데이터를 버리면됩니다. 그러나 만약 약하다면, 여러분은 더 많은 데이터가 필요합니다. 만약 더 많은 데이터를 갖추지 못한다면 — 이는 많은 연구 분야에서 공통적으로 일어나는 일입니다 — 여러분은 기존에 존재하는 데이터를 교차검증함으로서 사용하는 것이 더 나을 것입니다(이 지점에서  Do evaluate a model multiple times 를 살펴보십시오) . 여러분은 또한 데이터 증감(data augmentation) 기술을 사용할 수 있고, 이는 작은 데이터셋을 효과적으로 부스팅 할 수 있습니다. 데이터 증감은 여러분의 데이터셋의 특정 부분이 제한적인 상황에서도 사용할 수 있는데, 예를들어 특정 클래스의 샘플이 다른 것들보다 크게 적을때 — 이러한 상황은 클래스 불균형(class imbalance) 라고 불립니다 ( Haixiang et al. 에서는 이를 어떻게 다루어야 하는지에 대한 방법을 리뷰하고 있습니다; 또한 Don't use accuracy with imbalanced data sets 부분도 살펴보십시오). 그러나 여러분이 제한적인 데이터를 가지고 있다면, 여러분은 많은 파라미터를 가진 모델들의 ML 모델의 복잡도를 제한해야 한다고 생각하는 경향이 있습니다. 이 부분 역시, 앞서 발생한 이슈를 확인하는 것이 중요하고, 적당하거나 (방어적인) 전략을 사용하여 이를 완화해야 합니다. 

### 2.4 도메인 전문가와 이야기를 나누어 보십시오

도메인 전문가는 매우 귀중할 수 있습니다. 문제들을 효과적으로 풀어내기 위해 데이터를 이해하는 여러분을 도울 수 있으며, 더 적절한 피쳐셋과 ML 모델을을 고를 수 있게 도와줄 것이며, 더 적절한 오디언스에게 제출할 수 있도록 도와줄 것입니다. 도메인 전문가의 의견을 고려하는 것에 실패하는 것은, 유용하게 풀지 않거나 부적절한 방법으로 문제를 풀도록 유도할 수도 있습니다. opaque ML모델을 사용하여 어떻게 모델이 결과물을 내는지 반드시 이해해야 했던 상황(문제, problem)을 해결해야 했던 예가 있습니다. 프로젝트 시작할 당시, 도메인 전문가는 여러분이 데이터를 이해하고, (모델이) 예측 가능하도록 미래의 피쳐를 짚어줄 수도 있습니다. 프로젝트 마지막엔 그들은 도메인 특화 저널에 여러분의 논문을 제출할 수 있도록, 오디언스에게 전달될 수 있도록 도와줄 수 있는데, 이는 여러분의 연구에 큰 도움이 될 것입니다.

### 2.5 많은 논문과 문서들을 찾아보십시오

여러분은 특정 문제 도메인에 첫번째로 ML을 소개하는 사람은 아마 아닐 것입니다. 그러므로 이전에 어떤것이 진행되었고 그렇지 않았는지 이해하는 것은 굉장히 중요합니다. 다른 사람들이 동일한 문제를 진행한 것은 나쁜 것이 아닙니다; 학술적인 진보는 일반적으로 반복된 과정 속에서 일어나며, 각 연구에 제공된 정보들은 다음 연구에 대한 가이드가 될 것입니다. 다른 연구자가 이미 여러분의 좋은 아이디어를 진행한 것이 여러분을 낙담시킬 수 있지만, 그들은 여전히 조사할 만한 부분을 많이 남겨두었을 것입니다. 그리고 그들의 이전 연구는 여러분의 연구에 정당성을 부여할 수도 있습니다. 이전 연구를 무시하는 것은 잠재적으로 가치있는 정보를 놓치는 것과 같습니다. 예를 들어 누군가는 여러분이 제안한 접근방식을 예전에 진행하였을 수 있고, 왜 그것이 불가능한지 근본적인 원인을 찾았을 수도 있습니다(좌절할 몇년을 아껴준 셈입니다). 또한 여러분이 생성한 방식의 일부 문제를 이미 풀어두었을 수도 있습니다. 그러므로 연구 전에 논문 리뷰를 하는 것은 매우 중요합니다; 너무 늦게까지 놔둔다면, 이는 여러분이 논문을 쓸 때, 왜 동일한 증거 또는 이미 존재하는 증거를 사용하지 않았는지 설명하기 위해 힘겹게 할 것입니다.

### 2.6 여러분의 모델이 어떻게 이용될 지 생각해보십시오

ML모델을 왜 만들고 싶으십니까? 매우 중요한 질문이면서, 이 답은 모델을 개발하려는 여러분의 프로세스에 큰 영향을 줄 것입니다. 많은 학문적인 연구는 단지 — 연구— 라고 이야기 하는데, 이는 실제 세상에서 사용되는 모델을 생산하고자 한다는 의도와 멀리 떨어져있습니다. 모델을 생성하고 분석하는 과정은, 문제를 바라보는데 매우 유용한 통찰을 제공한다는 점에서 충분히 만족스럽습니다. 그러나, 많은 학술적인 연구에서, 결과적인 목표는 실제 상황에서 사용될 수 있는 ML모델을 생산하는 것입니다. 이러한 경우라면, 이 모델이 어떻게 이용되어씅면 하는지 먼저 생각하는 것은 매우 가치있습니다. 예를 들어서, 자원 제한적인 환경에 사용되기를 바란다면, 센서나 로봇과 같이 말입니다. 이는 모델의 복잡도에 일부 제한이 있을 것입니다. 밀리 세컨드 하에서 신호가 분류되어야 하는 등의 시간적 제약이 있다면, 모델을 선택할 때부터 이런 부분들이 충분히 설명되어야 합니다. 또 생각해보아야 할 점은, 모델이 사용되는 곳의 소프트웨어 시스템이 얼마나 넓은지에 대한 것입니다. 이러한 절차는 단순함과는 멀리 떨어져있습니다. 그러나 ML Ops와 같은 새롭게 등장하는 접근 방식은 이러한 어려움을 해결하고자 하는 목적을 가지고 있습니다.

---

## 3. 신뢰할 수 있는 모델을 만드는 방법

모델을 생성하는 것은 ML 부분에서 가장 재미있는 부분 중 하나입니다. 모던 ML프레임워크를 사용하면, 여러분의 데이터를 모든 접근 방법에 쉽게 던져넣을 수 있고 그 결과를 확인할 수 있습니다. 그러나, 이는 정의하기 어려운 실험의 조직화되지 않은 혼란을 야기할 수 있으며, 관련한 보고서를 작성하기도 어려울 수 있습니다. 그러므로, 조직화된 방법 속에서 모델을 생성하는 것, 여러분의 데이터를 올바르게 확인하는 것, 모델의 선택을 위한 적절한 고려를 하는 것이 중요합니다.  

### 3.1 학습 과정에서 테스트 데이터를 누출하지 마십시오

얼마나 여러분의 모델 일반화가 잘 되었는지 측정하기 위해 사용하는 데이터를 갖고 있는 것은 필수적입니다. 일반적인 문제들은 이러한 데이터가 configuration, 학습이나 모델 선택 시에 있어 누출되는 것을 허용한다는 것입니다. 이러한 것이 발생될 떄, 데이터는 더이상 신뢰할만한 일반화 측정을 제공할 수 없게 되며, 이는 왜 ML모델이 실제 세상 데이터에 일반화 되는 것에 종종 실패하는지의 주요 원인이 됩니다. 테스트셋에서 정보 누출이 발생하는 수 많은 방법이 있습니다. 몇몇은 꽤 innocuous 합니다. 예를 들어, 데이터 준비 과정에서 변수의 평균값과 전체 데이터의 다양한 변수 스케일에서 얻을 수 있는 범위에 대한 정보를 사용하는 때가 있는데 — 정보 부족을 예방하기 위함 — 이러한 종류의 정보들은 학습 데이터에서만 확인되어야 합니다. 다른 정보 누출의 일반적인 예시는, 데이터를 파티셔닝 하기전에 피쳐 선택을 하는 것입니다 (Do be careful when you optimise hyperparameters and select features 를 보세요). 그리고 다수 모델의 일반성을 동일한 테스트 데이터를 활용하여 평가하는 경우도 있습니다. (Do use a validation set and Don't always velieve results from community benchmarks를 보세요) 여러분이 이러한 문제를 예방하기 위해 가장 좋은 것은 여러분의 데이터 집합을 프로젝트 첫 시작부터 올바르게 분리하는 것입니다. 그리고 독립적인 테스트 셋은 프로젝트 마지막에서 단순 모델 일반성 수준을 측정하기 위한 방법으로만 사용하십시요 ( Do save some data to evaluate your final model instance 를 보세요) . 더 넓은 논의가 필요하다면, 아래 두 논문을 살펴보세요. 

### 3.2 다양한 모델의 범위를 사용하십시오

일반적으로, 최고의 단일 ML모델은 없습니다. 사실, 이를 증명하기 위해서는 '공짜 점심은 없다' 라는 이론을 빌려와야 하는데요, 이는 모든 가능성 있는 문제를 고려할 때, 어떤 것보다 항상 더 나은 ML접근이 없다는 것을 보여줍니다. 그래서 여러분의 업무는 여러분의 특정 문제에서 잘 동작하는 ML모델을 찾는 것에 있습니다. 이들 중에 밀접하게 관련된 문제의 좋은 품질의 연구 형태 하에서 priori 한 지식이 있는데요, 대부분의 시간동안 여러분들은 어둠속에서 작동하게 될 것입니다. 다행히도 파이썬의 모던 ML라이브러리 , R 등은 적은 코드 변화로 다수 모델을 사용할 수 있도록 도와줍니다. 그렇기에 여러분의 업무가 최고가 되기 위해 스스로 모델을 찾아내어 다양한 모델을 사용하지 말아야 할 이유는 없습니다. 공짜점심은 없다, 의 아래에서 '여기서 개발한 것이 아니다(not invented here syndrome) ' 라는 신드롬을 피하는 것도 굉장히 중요한데요, 예를 들어 여러분의 직관에서 개발된 모델만 사용하는것, 이는 특정 문제에 관한 최고의 모델이 몇가지를 생략할 수도 있기 때문입니다. 

### 3.3 부적절한 모델을 사용하지 마십시오

구현에 있어 장벽이 낮기 때문에, ML라이브러리는 여러분의 데이터에 부적절한 모델을 적용하기도 쉽게 만듭니다. 범주형 변수가 수치형 변수로 축약된 데이터셋이 되도록 모델을 적용하는 것, 변수에서 시계열 데이터 사이에 아무런 의존성이 없다는 가정을 하는 모델을 적용하는 것 들이 그 예시가 됩니다. 이는 결과를 보고하기 때문에 제출의 관점에서 특히 고려해야는 사항입니다다. 부적절한 모델은 리뷰어에게 이 모델에 대한 나쁜 인상을 줄 것입니다. 또다른 예시는, 모델을 불필요한 수준으로 복잡하게 사용하는 것입니다. 예를들어, 딥 뉴럴 넷트워크는 제한된 데이터를 사용한다면, 만약 도메인 지식이 하위 패턴이 꽤 단순하다고 제안한다면, 또는 모델이 해석될 필요가 있다면, 좋은 선택은 아닙니다. 마지막으로, 모델을 선택하는데 있어서 최근에 나온 모델을 당연하게 사용하지 마십시오. 오래되었고, 이미 형성된 그런 모델이 최근 모델보다 더 좋은 결과를 낼 수도 있습니다. 

### 3.4 여러분 모델의 하이퍼파라미터를 최적화하십시오

많은 모델은 하이퍼파라미터를 가지고 있습니다 — 즉, 모델의 configuration은 숫자나 셋팅에 영향을 받는다는 것입니다. 예를 들어, SVM 모델 속의 커널 함수, 랜덤 포레스트 속의 나무 갯수, 뉴럴 네트워크의 아키텍쳐 갯수들이 그렇습니다. 많은 이러한 하이퍼파라미터들은 중요하게 모델의 퍼포먼스에 영향을 줍니다. 그리고 일반적으로 하나가 전체에 맞지않습니다. 즉, 모델을 최대한 활용하려면 특정 데이터 셋트에 맞춰야 합니다. 하이퍼파라미터를 한개씩 움직이는 것이 유혹적일 수 있지만, 실질적으로 작동하는 것을 찾을때 까지 하이퍼파라미터를 사용하는 것은 최적의 접근 방식이 아닐 수 있습니다. 일종의 하이퍼파라미터 최적화 전략의 사용하는 것이 더 나을수 있으며, 이는 적어두었을때 정당화 하기가 훨씬 쉽습니다. 기본 전략은 랜덤 서치와 그리드 서치를 포함합니다. 그러나 대규모로 확장되지 않습니다. 학습에 비용이 많이 드는 모델이나 하이퍼 파라미터의 수보다 지능적인 방식으로 최적의 구성을 검색하는 도구 사용인 AutoML기술을 사용하여 최적화 할 수 있습니다. 이는 모델을 선택하고 다른 부분의 하이퍼파라미터를 선택하는데에(파이프라인 사용) AutoML기술을 사용하는 것도 가능합니다. 

### 3.5 하이퍼파라미터를 최적화하고 피쳐를 선택할 때 주의하십시오

모델 훈련의 또 다른 일반적인 단계는 피쳐 선택을 하는 것입니다. 그러나 두 하이퍼 파라미터 최적화를 모두 수행할 때 및 피쳐 선택을 수행할 때, 모델 교육 전에 수행하는 것 보다 일반적인 것이 아니라 모델 학습의 일부로 처리하는 것이 중요합니다. 특히 일반적인 오류는 모델 학습이 시작되기 전에 전체 데이터세트에 대한 기능 선택을 수행하는 것이지만, 이는 테스트셋에서 학습 프로세스가 정보 누출되는 결과를 낳습니다. 

모델에서 사용하는 하이퍼파라미터 또는 기능을 최적화 하는 경우 이상적으로 학습 데이터와 동일한 데이터를 사용해야 합니다. 이를 수행하는 일반적인 기술은 중첩 교차 검증(이중 교차 검증) 으로, 기본 교차 검증 루프 내부의 추가 루프로 초매개변수 최적화 및 기능 선택을 수행합니다. 더 광범위한 논의는 Cawley and Talbot 의 문서를 참조하세요. 교차 검증에 대한 자세한 정보를 위해 평가를 여러번 하십시오

---

### 4.어떻게 모델을 robustly 하게 평가할 수 있을까

여러분의 필드에서 개선에 대한 기여를 하기 위해, 여러분은 신뢰할만한 결론을 가진 검증 결과를 만들어야 할 필요가 있습니다. 안타깝게도 ML 모델을 잘못 평가하는 것은 쉽습니다. 그렇게 함으로써 학문적 진보의 물을 흐릿하게 만듭니다. 그래서 당신의 실험 속에서 여러분의 데이터를 어떻게 사용할 지 잘 생각해보십시오. 모델의 실제 퍼포먼스를 측정하기 위해 어떻게 할 것인가, 그리고 의미있고 정보가 많은 방법으로 리포트를 하기 위해 어떻게 진행할 수 있을까요.

### 4.1 적당한 테스트셋을 사용하십시오

우선, 항상 테스트 셋을 사용하여 ML모델의 일반성을 측정하십시오. 모델이 학습셋에서 얼마나 잘 수행되는지 아는 것은 의미가 거의없으며, 충분히 복잡한 모델은 훈련 셋을 완전히 학습할 수 있으나 일반화 가능한 지식은 포착할 수 없습니다. 테스트 셋의 데이터가 적절한지 확인하는 것도 중요합니다. 즉, 훈련 셋과 겹치지 않아야 하고 더 많은 인구를 대표해야 합니다. 예를 들어, 훈련 및 테스트 셋의 이미지가 화창한 날 야외에서 수집된 사진 데이터셋을 생각해 보십시오. 동일한 기상 조건의 존재는 테스트 셋이 독립적이지 않다는 것을 의미하며, 더 광범위한 기상 조건을 포착하지 않음으로써 대표성이 없을 것입니다. 훈련 데이터와 테스트 데이터를 모두 수집하는 데 단일 장비를 사용하는 경우에도 유사한 상황이 발생할 수 있습니다. 모델이 장비의 특성을 과도하게 학습하면 다른 장비로 일반화 되지 않을 가능성이 높으며 이는 테스트 셋에서 평가하여 감지할 수 없습니다.

### 4.2 검증 셋을 사용하십시오

학습 을 위해 다수의 모델을 연속적으로 학습하는 것은 일반적이지 않은데, 각 모델의 퍼포먼스에서 얻은 지식을 다음번의 configuration 의 가이드로 사용하여 말입니다. 그렇게 진행하게 될 경우, 이 프로세스에 테스트셋을 사용하지 않는 것은 중요합니다. 분리된 검증셋은 퍼포먼스를 측정하는데에만 사용되어야 합니다. 학습할때 직접적으로 사용되지 않았으면서 학습에 가이드를 주곤했던 샘플셋을 포함해야 합니다. 만약 의도적으로 테스트셋을 사용했다면, 테스트셋은 학습 프로세스의 암묵적으로 한 부분이 될 것이고, 예를 들어 여러분의 모델은 크게 테스트셋에 과대적합될 것입니다. 검증 셋을 따로 갖는 것의 또 다른 장점은, 여러분이 early stopping 을 할 수 있다는 점입니다. 이는, 단일 모델을 학습하는 도중에, 학습과정의 각 iteration마다 검증셋에 기반하여 모델을 측정할 수 있습니다. 모델이 학습 데이터에 과대적합 되기 시작하는 것을 알려주기 때문에, 학습은 검증 스코어가 떨어지기 시작할 때 멈추게 됩니다.

### 4.3 모델을 여러번 평가하십시오

많은 ML모델은 안정적이지 않습니다. 즉 여러분이 그 모델을 여러번 학습한다면, 또는 많은 학습 데이터에 작은 변화를 준다면, 그 퍼포먼서는 크게 달라질 것입니다. 이는, 단순한 모델 평가가 안정적이지 않을 수 있다는 것을 의미합니다. 또한, 모델의 실제 잠재력을 과소 평가하거나 과대평가 할 수 있습니다. 이러한 이유 때문에, 여러번의 평가를 수행하는 것은 일반적입니다. 이를 진행하는 것은 다양한데, 대부분은 학습 데이터의 다양한 하위 집합을 사용하여 모델을 여러번 학습하는 것을 포함합니다. Cross-validation(CV)는 특히 유명한데, 수많은 다양성을 가지고 있습니다. 10foldCV 이는 10번을 반복하여 학습한다는 것을 의미하는데, 논쟁할 것도 없이 일반적이지만, 여러분들이 반복된 CV를 사용하여 더 정확성을 높힐 수 있으며, 그리고 전체 CV는 데이터의 다양한 파티셔닝을 가지고 여러번 반복하는 작업을 의미합니다. 만약 여러분의 데이터중 일부 클래스가 작다면, 이때 계층화 작업을 수행하는 것이 필요합니다. 이는 각 클래스를 각 fold 에 적합하게 표현합니다. 여러 평가지의 평균과 표준편차를 보고 하는 것이 일반적이나, 나중에 통계 테스트를 사용하여 모델을 비교할 경우를 대비하여 개별 점수를 기록해 두는 것이 좋습니다.  (Do use statistical test when comparing models 를 보세요)

### 4.4 최종 모델 인스턴스를 평가하기 위한 일부 데이터를 저장해두십시오

model이라는 단어를 꽤 일반적으로 사용하곤 하는데, 일반적인 모델의 잠재력을 평가하는 것(예를 들어, 뉴럴 네트워크가 여러분의 문제를 얼마나 잘 풀 수 있는지) 과 특정 모델 인스턴스의 퍼포먼스 ( 특정 뉴럴 넷트워크는 역전파를 통해 진행된다) 교차 검증은 앞선 경우에 잘 들어맞으나, 후자의 경우 유용하지 않게 됩니다. 예를 들어 여러분이 10fold 교차 검증을 수행한다고 생각해보겠습니다. 10개 모델 인스턴스에 대한 결과물이 있을 것입니다. 가장 높은 테스트 폴드 스코어를 보이는 인스턴스를 선택하여 실전에서 사용할 것입니다. 이 성능을 어떻게 보고할 것인가요?  테스트 폴드 점수가 퍼포먼스를 측정하는데 신뢰할만하다고 생각하겠지만, 실제로 그렇지 않습니다. 먼저, 단일 폴드의 데이터 양은 상대적으로 매우 작습니다. 두번째로 가장 높은 점수의 인스턴스는 가장 쉬운 테스트 폴드 중 하나일 것입니다. 그래서 평가 데이터는 아마 대표성을 가지고 있지 못할 것입니다. 결론적으로, 신뢰할만하다고 예측되는 모델의 일반성을 얻기 위한 방법은 다른 테스트셋을 사용하는 것이다. 그래서 만약 여러분이 충분한 데이터를 가지고 있다면, 한쪽으로 밀어두고, 최종 모델 인스턴스 선택시, bias없는 추정할 때만 사용할 것입니다.

### 4-5. 불균형 데이터 셋에 accuracy를 사용하지 마십시오

여러분의 ML모델을 평가하는데에 있어 사용하는 메트릭에 주의를 기울여야 한다. 예를 들어, 분류 모델의 경우, 대부분의 일반적으로 사용되는 메트릭은 정확도인데, 이는 모델에 의해 정확하게 분류된 데이터셋 내 샘플의 비율을 의미한다. 여러분의 클래스가 균형적이라면 잘 작동할 것이다, 예를 들어 각 클래스가 비슷한 샘플의 수를 표현한다고 할때 말이다. 그러나 대부분의 데이터는 균형이 맞추어져 있지 않습니다. 그리고 이러한 경우 정확도라는 지표는 잘못된 결과를 가져올 확률이 높습니다. 예를 들어, 90%의 데이터는 하나의 클래스를 나타내고, 10%는 또 다른 클래스를 나타내는 데이터셋이 있다고 생각해보자. 이진 분류기는 어떤 데이터가 들어오더라도 언제나 첫번째 클래스를 결과물로 낸다면, 그 모델이 완전히 필요가 없음에도 불구하고 정확도는 90%가 될 것이다. 이러한 경우, Cohen's kappa coefficient (k) 나 Matthews Correlation Coefficient(MCC) 를 사용하는 것이 더 좋은데, 각각은 클래스 사이즈의 불균형에 대해 상대적으로 덜 민감합니다 ( Do report performance in multiple ways를 살펴보시오) 불균형 데이터를 다루는 더 넓은 방법 리뷰가 궁금하다면 Haixiang et al. [2017] 논문을 살펴보세요.

---

## 5. 모델을 어떻게 공정하게 평가할 것인가

아카데믹 연구의 기본은 모델 비교입니다. 그러나 놀랍게도 이를 제대로 하는 것은 어렵습니다. 만약 부정확한 비교를 하고, 논문을 제출한다면, 다른 연구자들은 잠재적으로 길을 잃게 될 것입니다. 그래서 여러 모델을 동일한 맥락에서 평가하였는지를 확인하세요.  다수의 관점을 탐색하고, 통계적 테스트의 사용이 올바르게 되었는지를 체크하라.

### 5.1 큰 숫자가 언제나 더 나은 모델을 의미하는 것은 아닙니다.

"이전 연구에서, 정확성이 94%까지나 상승했다. 우리 모델은 95%이다. 그러므로 더 나아졌다" 라고 말하는 것은 일반적이다. 다양한 이유로, 왜 큰 숫자가 더 나은 모델이라고 말하지 않는지 보여줄 것이다. 예를 들어, 모델이 동일한 데이터셋의 다른 파티션으로 학습되거나 평가되었다면, 퍼포먼스의 작은 차이는 이 부분에서 발현되었을 수 있다. 만약 전체적으로 다양한 데이터셋을 사용한다면, 퍼포먼스에 있어서 큰 차이를 보이는 원인이 될 수도 있다. 불공정한 비교의 또 다른 이유는 파라미터 최적화 양을 동일하게 하는 것을 실패하였다고도 볼 수 있다. 예를 들어, 한 모델은 단순히 기본 셋팅만 하고, 다른 모델은 최적화 하였을 경우, 그 비교는 정당하지 않습니다. 이러저러한 이유들 때문에, 제출된 논문 숫자에 기반한 비교는 언제나 주의를 요합니다. 두가지 접근 사이에 합당한 비교인 것이 정말 확실시 된다면, 여러분이 비교하는 전체 모델을 새롭게 구현해야 하고, 동일한 수준으로 각각 최적화 해야 하며, 다수의 평가를 만들어 내고,  퍼포먼스 하의 다양성이 큰지 작은지 결정하기 위해 통계적 테스트를 진행하십시오.

## 5-2. 모델 비교 시 통계적 테스트를 사용하십시오

만약 여러분의 모델이 다른 이의 모델보다 더 낫다고 설득하고 싶다면, 통계적 검증은 매우 유용한 도구가 됩니다. 크게 이야기 해보자면, 각각 ML 모델을 비교하는 데에는 2개의 카테고리가 있다. 첫번째는 각 모델 인스턴스를 비교하는 것을 의미하는데, 2개 학습된 의사결정나무 들이 그렇습니다. 예를 들어, McNemar's 검정은 2개의 분류기를 비교하는데에 꽤 괜찮은 선택 입니다. 그리고 테스트셋 속의 각 샘플에 대한 라벨 아웃풋을 비교하기에도 좋습니다. (그래서 여기서 나온 수치를 기억하세요) 테스트의 두번째 카테고리는 더 일반적으로 두 모델을 비교하는데 사용됩니다. 예를 들어 의사결정 나무와 뉴럴 네트워크는 데이터에 더 잘 적합하게 됩니다. 각 모델의 다수 평가가 필요한데, 이는 교차 검증이나 반복된 리샘플링을 통해 진행가능하고(또는 만약 여러분의 학습 알고리즘이 확률적이라면(stochastic) 하다면, 동일한 데이터로 몇 번의 반복을 할 수 있습니다.) 테스트는 두 결과 분산으로 비교할 수 있습니다. T 검정은 비교할 때 가장 많이 사용됩니다. 그러나 두 분산이 정규 분포일때만 신뢰할 수 있는데, 이는 일반적인 경우는 아닙니다. 더 안전한 방식은 만-휘트니 U 검정인데, 이는 두 분산이 정규 분포가 아니라는 것을 가정한다. 더 많은 정보를 접하기 위해서는 아래를 살펴보길 바랍니다. 

[Raschka, 2020] 
[Carrasco et al., 2020].
Do correct for multiple comparisons 
Do be careful when reporting statistical significance.

### 5-3 다중 비교를 수정하십시오

여러분이 통계 검정을 사용하여 두 개 이상의 모델을 비교할 때, 더 복잡해지는 경우가 있습니다. multiple pairwise 검정의 경우 검정셋이 여러번 사용됩니다 - 이는 중요도의 해석이 널리 긍정적일 수 있다. 일반적으로, 여러분이 두 모델을 통계적 검정을 사용하여 비교할 때마다, 둘 사이에 큰 차이가 없다고 발견될 가능성도 있습니다. 검정의 신뢰수준 에 의해 표현될 수 있는데, 대부분 95%에 셋팅을 둡니다: 이는 20번 중 1회 정도 잘못된 긍정을 할 수 있다는 것입니다. 단일 비교에 있어서, 여러분들이 만들 수 있는 불확실성의 수준입니다. 그러나, 이러한 것이 축적된다면 어떨까. 즉 여러분들이 95%의 신뢰수준에서 20번의 pairwise 검정을 수행하고, 그중에 하나가 틀린 답을 낸다면 어떨까요. 이를 multiplicity effect 다양성의 효과라고 말할 수 있는데, 데이터 사이언스에서 널리 퍼져있는 이슈의 예시중 하나인 data dredging 이나 p-hacking 이 그러합니다. 이 문제를 해결하기 위해, 다수의 검정을 고쳐잡고자 적용해야 합니다. 가장 일반적인 접근은 Bonferroni correction 인데, 매우 단순한 방법이며, 수많은 검정에 기반하여 중요한 threshold를 낮게 하는 것이다. [Salzberg, 1997] 이 논문에 잘 설명되어있습니다. 그러나, 다른 접근들도 많으며, 언제 그리고 어디에서 이러한 교정이 적용되어야 하는지도 여전히 논쟁중입니다. 더 보고자 한다면 이 논문을 보세요 [Streiner, 2015].

### 5-4. 커뮤니티 벤치마크의 결과를 언제나 신뢰할 필요는 없습니다

특정 문제 도메인에서는, 벤치마크 데이터 셋을 새로운 ML모델 평가하는 것에 사용하는 게 일반화되었다. 이러한 아이디어는, 모든 사람들이 동일한 데이터를 그들의 모델 학습과 검정에 사용하기 때문에, 비교가 더 투명하기 때문입니다. 하지만 안타깝게도 이러한 접근 방법은 큰 결점을 가지고 있습니다. 먼저, 테스트 셋이 제한적이지 않다면, 여러분은 사람들이 학습 과정의 일부분에 그 데이터를 사용하였는지 가정할 수 없습니다. 이는 테스트셋의 발전 (developing to the test set) 이라고도 알려져있는데, 과한 긍정의 결과를 보입니다. 더 미묘한 문제가 있는데, 심지어 모든 사람들이 테스트셋으로만 단 한번 그 데이터를 사용하였였지만, 테스트셋은 커뮤니티에 의해 여러번 사용되었습니다. 사실상, 많은 모델을 비교하는데 있어서 최고 모델은 테스트셋에 오버핏 되는 경향이 점점 증가하고 있고, 다른 모델보다 더 나은 일반화가 필요하지도 않습니다. (Do correct for multiple comparison을 보라) 그렇기 때문에, 다른 이유때문에, 벤치 마크 데이터셋 결과를 얼마나 많이 읽었던 간에 조심해야 하고, 퍼포먼스에서의 작은 증가가 중요하다는 점을 가정하지 말아야 합니다다. 공유된 데이터셋 사용에 대한 여러 이슈들과 더 넓은 논의를 위해서는 아래 두 문서를 살펴보십시오.
[Paullada et al., 2020]
Do report performance in multiple ways.

### 5-5. 모델 조합을 고려하십시오

이 섹션은 모델 비교에 중점을 두고 있으나, ML 이 항상 다른 모델 중에서 선택하는 것은 아니라는 점을 인식하는 것이 좋습니다. 가끔 모델의 조합을 사용하면 이해할 수 있습니다. 다양한 ML 모델은 다양한 trade-off 를 보여주는데; 그들을 조합함으로서, 여러분들은 한 모델의 약점을 다른 모델의 강점으로 보충할 수 있게 되거나, 혹은 그 반대가 일어납니다. 그러한 구조 모델들은 앙상블이라고 알려져있으며, 그들을 생성하는 과정을 알상블 학습이라고 부릅니다. 앙상블 학습 접근에는 다양한 것들이 있습니다. [Dong et al., 2020] 에서 그 리뷰를 살펴볼 수 있습니다. 그러나 동일한 기본 모델 타입의 앙상블 형태로 대략적으로 나누어집니다. 예를 들어 의사결정나무의 앙상블과 ML 모델의 다양한 종류를 합치는것들, 또는 의사결정나무, SVM, 깊은 뉴럴 네트워크를 합치는 것들 말입니다. 첫번째 카테고리는 많은 클래식한 접근을 포함하고 있는데, 배깅과 부스팅이 그렇습니다. 앙상블은 기존의 훈련된 모델에서 형성되거나 기본 모델이 프로세스의 일부로 훈련될 수 있으며 일반적으로 데이터 공간의 다른 부분에서 실수를 범하는 다양한 모델을 생성하는 것을 목표로 합니다. 앙상블 학습의 일반적인 고려는 어떻게 다른 기초 모델을 합칠것인지에 대한 것입니다; voting 과 같은 매우 단순한 방법에서부터, 다른 ml 모델을 기본 모델의 결과에 통합하기 위해 더 복잡한 접근 방법을 사용하는 것까지 다양합니다. 후자 접근방식은 보통 stacking 이나 stacked generalisation 이라고 불립니다. 

---

## 6. 어떻게 결과를 정리할 것인가

학술 연구의 목적은 자기계발이 아니라, 지식에 기여하는 기회입니다. 효과적으로 지식에 기여하기 위해서, 여러분들은 여러분 업무, 어떤 것을 진행했고 어떤 것을 하지 않았는지에 대한 완벽한 사진을 제시해야 할 필요가 있습니다. ML은 trade-off와 관련되는데 — 하나의 모델이 어떤 다른 모델보다 엄청나게 우수한 경우는 매우 희귀합니다 — 그리고 결과 및 결론을 보고하는데 있어 미묘하고 신중한 접근 방식으로 이를 반영하려고 노력해야 합니다. 

### 6-1. 투명해지십시오

먼저, 여러분이 무엇을 했는지 무엇을 발견했는지에 대해 투명하게 공개하려고 하십시오. 이는 여러분 업무 덕분에, 다른사람들에게는 더 쉽게 느껴질 것입니다. 특히, 여러분의 모델을 접근 가능한 방식으로 공유하는 것은 매우 실용적입니다. 예를 들어, 만약 여러분의 실험에서 구현하기 위해 스크립트를 사용하였다면, 그 스크립트를 여러분의 결과 논문 발표시에 같이 공개하십시오. 이는 다른 사람들이 여러분의 실험을 쉽게 반복할 수 있다는 의미이며, 이는 여러분의 업무에 자신감을 더해줄 것입니다. 또한 사람들이 쉽게 모델을 비교할 수 있게 해주십시오. 사람들은 모든 것들을 공정한 비교를 위해 재구성할 수 있는 여력이 없기 때문입니다. 여러분의 업무가 공유될 수 있다는 것을 아는 것은 여러분이 문서와 실험, 코드를 적는것에 대해 더 신중할 수 있도록 도와줄 것이며 이는 여러분 뿐 아니라 다른ㄹ 사람들에게도 좋을 것입니다. 재생산성과 관련된 이슈들이 ML 커뮤니티 안에서 유명해지는 것도 가치가 있습니다. 그러므로, 미래에 여러분은 적절한 문서와 공유된 워크플로우가 없으면, 논문을 내지 못할 수도 있습니다. 예시는 이 문서를 보십시오. [Pineau et al., 2020].

### 6-2. 다양한 방법으로 퍼포먼스를 보고하십시오

모델을 평가하고 비교할 때, 더 엄밀하게 하는 한가지 방법은 여러 데이터셋을 사용하는 것입니다. 이는 개별 데이터셋의 결합된 어떠한 결핍도 극복하게 도와줍니다 (Don't always believe results from community benchmarks 참조) 그리고 여러분에게 여러분의 모델 퍼포먼스의 더 복잡한 그림을 보여줍니다. 각 데이터셋을 위한 다양한 매트릭을 보고하는 것은 실용적인데, 그 이유는 다양한 메트릭은 결과에 대한 다양한 시각을 제공해주기 때문이고, 여러분의 업무의 투명성을 증가시켜주기 때문입니다. 예를 들어 여러분이 accuracy를 사용한다면, 이는 클래스 불균형에 덜 민감한 평가지표를 포함하는 것도 좋은 생각이 될 것입니다. (Don't use accuracy with imbalanced dataset 참조) 만약 여러분이 precision, recall, sensitivity 나 specificity 를 부분적으로 사용한다면, 또한 여러분의 모델 오류 비율의 완벽한 그림을 제공하는 지표를 추가할 수도 있습니다. 또한 어떠한 지표를 여러분이 사용하는지 확실하게 하십시요. 예를 들어, 만약 F-score를 보고할 때, F1인지 아닌지 명확히 하십시오. 또는 precision 과 recall 간의 균형을 체크하십시오. 만약 AUC 를 사용한다면, 이는 PR 커브 또는 ROC 커브 아래 면적을 뜻하는 것인지 정확하게 이야기 하십시오. 더 넓은 논의를 위해서는 아래 논문을 참조하십시오.
[Blagec et al., 2020].

### 6-3. 데이터를 너머 일반화 하지 마십시오

타당성 없는 결론을 내지 않는 것은 중요한데, 그 이유는 다른 연구자들을 혼란에 빠뜨리기 때문입니다. 일반적인 실수는, 자신이 사용한 학습 / 평가 모델에 의해 추론할 수 없는 일반적인 결론을 만들어 내는 것입니다. 예를 들어, 여러분의 모델이 한 데이터셋에 완전하게 작동하지 않는다면, 이는 다른 데이터 셋에 잘 될 것이라는 것을 의미하지 않습니다. 다수의 데이터셋을 사용함으로서 견고한 인사이트를 얻을 수 있지만 (Do report performance in multiple ways) 어떤 실험적인 연구에 의해 여러분이 언급한 어떤것도 제한적일 수도 있습니다. 수많은 이유들이 있는데, 대부분의 것들은 어떻게 데이터셋이 만들어졌는지에 따라 움직입니다. 샘플링 오류나 bias가 포함된 데이터는 하나의 예시인데, 이는 실제 세상의 데이터를 충분하게 표현하는 데이터는 아닙니다. 또 다른 것은 중복입니다: 다수 데이터셋은 독립적이지 않기 때문에, 유사한 bias들이 포함되어있습니다. 품질 문제도 있습니다. 이는 딥러닝 데이터셋의 특정 문제입니다. 데이터 양이 필요하기 때문에 수행할 수 있는 품질 검사의 양이 제한됩니다. 간단히 말해서, 결과를 과장하지 말고 한계를 인식하십시오. 


### 6-4. 통계적 중요성을 보고할때는 신중하십시오

통계 검증에 대해서 이미 한번 이야기를 했고 (Do use statistical tests when compaing models 참조), 그리고 ML모델 간의 다른점을 결정하는데에 어떻게 사용하는지 말했습니다. 그러나, 통계적 검증은 완벽하지 않습니다. 일부는 보수적이며 중요성을 과소평가하는 경향이 있습니다. 또 다른 것들은 너무 자유로우며 중요성을 과대평가 하는 경향이 있습니다. 이는 긍정적인 검증이 언제나 중요한 어떤것을 뜻하지는 않는다는 것을 의미합니다. 또한 부정적인 검증이 언제나 중요하지 않은 것을 의미하는 것은 아님을 말합니다. 대신, 중요성을 결정하는데에 threshold 를 사용하는 것도 하나의 큰 이슈입니다; 예를 들면, 95%의 신뢰구간 threshold( p-value < 0.05인 지점) 은 20번 중에 1번은 다른 중요성 flag 를 가지고 있다는 것을 말하며, 이는 중요하지 않습니다. 사실 통계학자들은 threshold를 사용하지 않는 것이 더 나을지도 모른다는 논쟁을 벌이고 있으며, p-value 를 단순히 보고만 하거나 해석할때 리더에게만 제공하는 것이 의미있다고 이야기 합니다. 통계학적 중요성을 넘어, 또 다른 것은 두 모델의 차이점이 중요합니다. 만약 충분한 샘플을 가지고 있다면 중요한 차이를 발견할 수 있을 것입니다. 그러나 실제의 차이는 아주 작고 사소합니다. 어떤 것이 중요한지 여부를 더 잘 나타내기 위해 효과 크기를 측정할 수 있습니다. 이를 위해 사용되는 접근방식은 다양합니다. Cohen's d 통계량이 가장 일반적이지만 Kolmogorov-Smirnov와 같은 보다 강력한 접근방식이 더 좋습니다. 이에 대한 자세한 내용은 [Betensky, 2019].를 참조하십시오

### 6-5. 여러분의 모델을 살펴보십시오

학습된 모델은 많은 유용한 정보를 담고 있습니다. 안타깝게도 많은 저자들은 학습된 모델의 퍼포먼스 지표만 보고합니다. 어떤것을 실제로 배웠는지에 대한 인사이트를 제공하는 것 없이 말입니다. 연구의 목적은 다른사람들 보다 약간 더 높은 정확도를 얻는 것이 아니라는 점을 기억하십시오. 오히려 지식과 이해를 만들고, 이를 연구 커뮤니티와 공유하는 것입니다. 당신이 이것을 할 수 있다면, 당신은 당신의 연구에서 괜찮은 논문을 얻을 가능성이 훨씬 더 노습니다. 따라서 모델 내부를 살펴보고 결정에 도달하는 방법을 이해라려고 노력하십시오. 의사결정 13 나무와 같은 비교적 단순한 모델의 경우, 모델의 시각화를 제공하는 것도 도움이 될 수 있으며, 대부분 라이브러리에는 이를 수행하는 기능이 있습니다. 심층 신경망과 같은 복잡한 모델의 경우, 설명가능한 AI(XAI) 기술을 사용하여 지식을 추출하는 것을 고려하십시오. 모델이 무엇을 하는지 정확히  알려줄 수는 없으나 유용한 통찰력을 제공할 수 있습니다. 

---

## 7. 마지막 생각

이 문서는 여러분들이 알아야 할 모든것을 말하고 있지는 않습니다. 몇몇은 결론이 없었고, 또 다른 것들은 제가 말한 것이 틀렸을 수도 있으며, 적어도 논쟁가능성이 있을 수 있습니다. 제가 걱정하는 것은 연구의 본질입니다. ML을 수행하는 방법에 대한 이론은 거의 항상 실제보다 뒤쳐져 있으며, 학자들은 항상 최선의 작업 방식에 대해 의견이 일치하지 않으며, 오늘날 우리가 옳다고 생각하는 것이 내일 옳지 않을 수도 있습니다. 따라서 연구의 다른 측면과 거의 동일한 방식으로 ML에 접근해야 합니다. 열린 마음으로, 최근 개발을 따라가려는 의지, 모든 것을 알지 못한다는 사실을 겸손하게 받아들이는 것입니다.
