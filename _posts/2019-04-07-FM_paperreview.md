---
layout: post
title: "Factorization Machines"
author: "Seriouscoding"
---

추천시스템 스터디를 위한 논문 번역본이며, 오역이 있을시 말씀주시면 수정하겠습니다.

**출처 : Factorization Machines - Steffen Rendle**

______________________________________________________________

## 요약


이번 논문에서, Factorization Machines, 새로운 모델 클래스를 설명할 것이다. 이것은 Support Vector Machines (SVM) 의 장점을 factorization 모델에 합친 것이다. SVM 처럼, FM은 실제 가치있는 feature 벡터를 갖는, 일반적인 예측기 이다. 그렇기에, 추천 시스템과 같이 (SVM이 불가능한)높은 희소성을 보이는 문제에서도 상호작용을 예측하는 것이 가능하다. 우리는 FM 모델 방정식이 선형 시간 속에서 계산될 수 있다는 것을 보여주어, FM이 바로 최적화 될 수 있다는 것을 이야기 할 것이다. 비선형적인 SVM과 달리, 두가지 형태 속의 변형은 필수적이지 않으며, 그 해결 속에서 모델 파라미터들은 다른 서포트 벡터의 도움 없이 즉시 평가될 수 있다.  
Matrix factorization, parallel factor analysis, 또는 특화된  SVD++, PITF or FPMC. 모델들과 마찬가지로,  factorization models 은 많은 문제점을 가지고 있다. 이 모델의 결점은, 그들이 다른 예측 과제들에 이용할 수 없다는 점이다. 그러나, 특별한 input data 에는 사용 가능하다. 더욱이, 그들의 모델 방정식과 최적화 알고리즘은 각 과제 별 개별적으로 파생된다. 우리는 FM을 통해 input data 를 특정하기만 함으로써 저 모델들을 따라할 수 있다는 것을 보여줄 것이다 (즉, feature vector) 이는, FM 이 factorization model 에 대한 전문가적 지식 없이도 쉽게 이용할 수 있다는 것을 말한다. 

**Index Terms – 
factorization machine; sparse data; tensor factorization; support vector machine**


## 1. 소개 INTRODUCTION


Support Vector Machines 은 머신러닝과 데이터 마이닝 분야에서 가장 유명한 예측기 이다. 협업 필터링(collaborative filtering)과 같은 셋팅에도 불구하고, SVM 은 중요한 역할을 하지 않고, 최고의 모델들은 기본 matrix, tensor factorization model 과 같이  즉각적인 이용이 가능한 PARAFAC[1]이나, factorized 된 파라미터[2], [3]. [4]를 사용한 특별한 모델을 말한다. 이번 논문에서, 우리는 왜 기본 SVM 예측기가 이번 과제에서 성공적이지 않은지에 대한 이유를 제시한다. SVM 예측기는 신뢰할 수 있는 파라미터(‘hyperplanes’)를 복잡한(선형이 아닌) 커널 공간 속의 매우 희소성 있는 데이터 하에서 학습하지 못하기 때문이다. 반대로, 특화된 factorization 모델에 대한 tensor factorization 모델의 결점은, (1) 그들은 기본 예측 데이터 (예를들면, R의 n 제곱 크기의 실제 가치 피쳐 벡터) 를 이용할 수 없고 (2) 특화된 모델은 주로 모델링과 학습 알고리즘 설계를 요구하는 특별한 과제에 개별적으로 파생된다.
  이번 논문에서, 우리는 새로운 예측기인 Factorization Machine (FM)을 소개한다. 이 예측기는 SVM 만큼이나 일반적인 예측기 이면서, 매우 높은 희소성을 보이는 데이터 하에서도 신뢰할 만한 파라미터들을 평가할 수 있다. 이 FM 모델은 다양한 상호작용이 모두 중첩되었으며, (SVM 내 다항식 커널과 비교할 수 있다) SVM 처럼 인자화 된 매개 변수 (factorized parametrization) 를 밀접한 매개 변수(dense parametrization) 대신에 사용할 수 있다.  FM 모델 방정식이 선형 시간을 계산할 수 있고, 이는 파라미터의 선형 수치로 가능하다는 것을 말한다. 이는 직접 최적화를 가능하게 하고, 예측을 위한  다른 학습 데이터(support vectors)를 저장해둘 필요 없이, 모델 파라미터 저장을 용이하게 한다. 이와 반대로, 비선형 SVM은 주로 두가지 형태로 최적화 되고, 학습 데이터의 한 부분에 의존하여 (모델 방정식) 예측을 계산한다.( support vector) 우리는 또한 FM이 MF, SVD++ [2], PITF [3] and FPMC [4] 등의 협업 필터링 과제에 대해 많은 성공적인 접근들을 포괄하는 것을 보여준다.  
  정리하자면, 우리가 제안하는 FM은:
  1) FM은 SVM 이 실패하는, 매우 희소한 데이터 하에서도 파라미터 예측이 가능하다
  2) FM은 선형 복잡성을 가지고 있는데, 이는 초기에 최적화 가능하게 하고 SVM와 같이 support vector에 의존하지 않는다. 우리는 1억개 수준의 학습 인스턴스를 가진 넷플릭스와 같은 큰 규모 데이터로 FM 을 보여준다.
  3) FM은 기본적인 예측기 이며, 다른 실제 가치있는 피쳐 벡터 (real valued feature vector) 와 함께 작업 가능하다. 이와 반대로, 다른 최첨단의 factorization model은 매우 제한된 인풋 데이터로만 진행 가능하다. 우리는 인풋 데이터의 feature vector를 정의함으로써, FM이 최첨단 모델, 편향된 MF, SVD++, PITF or FPMC 만큼 효과를 볼 수 있다는 것을 보여줄 것이다. 


## 2. PREDICTION UNDER SPARSITY 희소성 하에서 예측하기


  대부분의 예측 업무는 real valued feature vector 에서, 타겟 도메인 T 을 달성하기 위한  (예를 들어, 회귀가 목적이라면, 분류가 목적이라면 ) 함수 를 예측하는 것이다. 지도 (학습) 셋팅에서, 학습 데이터 와, 타겟 함수 y 가 주어졌다고 가정할 수 있다. 우리는 타겟 이  feature vectors X 의 점수를 매기고, 그 점수에 따라 정리가 되는, 등급 매기기 과제를 조사하였다. 스코어링 함수는 학습 데이터 [5]와 짝을 맞춰 학습될 수 있는데, feature tuple 의 의미는 다음과 같다, 는 보다 높은 등급을 달아야 한다. 짝을 이룬 랭킹 관계가 좌우 대칭임에 따라, 긍정적인 학습 인스턴스만 사용하기 충분하다. 
이 논문에서, 우리는 X가 크게 희소하다면, 즉 거의 모든 벡터의 X 원소가 0일때의 문제를 다룰 것이다. feature vector X 의 함수의 원소가 0이 아니고, 모든 벡터 X가 D에 포함될 때, 원소  의 평균 숫자를 라고 두자. 많은 이벤트 트랜젝션 ( 추천 시스템에서의 구매) 이나 텍스트 분석 (bag of word 접근) 과 같은 실생활 데이터는 큰 희소성을 보인다. 높은 희소성이란 근본 문제가 큰 카테고리의 변수 도메인을 다루는 것을 의미한다.

_Example 1_
영화 리뷰 시스템 내 거래 데이터를 가지고 있다고 가정하자. U에 포함된 유저 한명이 I 에 속한 영화(아이템) 을 특정 시간 에 로 등급 매기는 것을 기록한다. 유저 U와 아이템 I는 아래와 같다:

관찰된 데이터 S는 아래와 같이 말할 수 있다. 

이 데이터를 사용하여 예시들은 함수 y 로 예측할 수 있다. 함수 y는 유저 한명이 아이템 한 개에 특정 시간 포인트에 등급 매기는 행동을 예측한다. 
Figure 1에서는 어떻게 feature vectors 가 S 에서 생성될 수 있는지 보여주는 사례이다. 여기, 첫번째 이진 지표 변수 (blue)가 있다. 이것은 거래의 활성 유저를 의미한다. - 각 거래  에는 한 명의 활성 유저가 반드시 존재한다. 유저 Alice 는 첫번째이다. 다음 이진 지표 변수 (red) 는 활성 아이템을 의미한다 - 활성 아이템 한 개는 반드시 존재한다.  figure 1 의 feature vector는 또한 이미 평가된 영화들을 위해 지표 변수(yellow)를  포함한다. 각 유저들에게, 변수 합이 1이 될 수 있도록 정규화 된다. (예를 들면, Alice 는 Titanic, Notting Hill, Star Wars를 선택했다)  추가적으로, 예시는 2009년 1월부터 월 단위 시간을 나타내는 변수 (green)을 포함한다. 마지막으로, 변수 (brown)은 한 명의 유저가 등급 매기기 전의 최근 활성화 영화를 의미한다 - X 에서, Alice 는 Notting Hill을 평가하기 전에 Titanic을 선택했다. Section V에서, 우리는 그러한 feature vectors 들이 인풋 데이터로 사용됨과 함께, 어떻게 factorization machines 이 특별한 최신 factorization 모델들과 관련이 있을 지 보여준다.

  우리는 설명을 위해 이번 페이지 전체를 데이터 설명에 사용할 것이다. 그러나, FM이 SVM과 같은 일반적인 예측기이고, 다른  real valued feature vectors 에 적용할 수 있고, 추천 시스템에 제한이 없다는 것을 잊지 말라.


## 3. FACTORIZATION MACHINE (FM)


  이번 section에서, 우리는 factorization machine을 소개한다. 우리는 모델 방정식을 주체적으로 이야기 하고, 어떻게 FM이 몇몇의 예측 과제에 적용될 수 있는지 간단하게 보여줄 것이다. 

### A. Factorization Machine Model
 
 1)	Model Equation : 차수 d = 2일때, factorization machine을 위한 모델 방정식은 아래와 같이 정의된다. 

우리가 예측해야 하는 모델 파라미터는 이것이다. 

그리고, < ∙, ∙ >는 k크기의 두 벡터의 내적을 의미한다. 

  V안의 vi행은 k factor 내 i 번째 변수를 의미한다.  는 factorization 의 밀도를 정의하는 하이퍼 파라미터를 의미한다. 
2방향 FM (차수 d = 2) 는 변수 사이의 모든 단일, 쌍방의 상호 작용을 보여준다:
●	는 전체 편차를 의미한다
●	 i 번째 변수의 강도로 모델링 한다.
●	i 와 j번째 변수 사이의 상호작용을 모델링한다. 모델이 가지고 있는 파라미터인 를 사용하는 것 대신, FM는 factorizing 함으로써 상호작용을 모델링 한다. 나중에 볼 부분이지만, 높은 품질의 파라미터가 희소성 하에서 고차원 상호작용 예측을 할 수 있게 하는 키포인트이다. 

  2) Expressiveness 표현력
  양의 한정 행렬(positive definite matrix) W로도 잘 알려져 있는데,  W =  는 k가 충분히 크다는 것을 의미하는 행렬 V가 존재한다. 이는, k가 충분히 큰 숫자라면, FM이 어떠한 상호작용 행렬 W을 표현할 수 있다는 것을 말한다. 희소한 수준임에도 불구하고, 전형적으로 작은 k는 선택되어야 하는데, 그 이유는 복잡한 상호작용 W를 예측하기에 충분한 데이터가 없기 때문이다. k를 제한하는 것 - 그래서 FM 표현력은 -  더욱 우수한 일반성과 희소성 하에서 개선된 상호작용 행렬을 유도한다.
  
  3) Parameter Estimation Under Sparsity 희소성 하에서 파라미터 예측
      희소한 상태에서, 변수간에 직접적이고 독립적으로 상호 작용을 예측하기에 충분한 데이터는 없는 것이 일반적이다. Factorization machine은 이러한 셋팅 속에서도 상호작용을 잘 예측할 수 있는데, 그 이유는 그것들이 상호 파라미터의 독립성을 factorizing 함으로써 무너뜨리기 때문이다. 일반적으로, 이는 한개의 상호작용 데이터가 관련된 상호작용을 위한 파라미터를 예측하는 데에도 도움을 준다는 것이다. 우리는 figure 1 속의 데이터를 통해 얻은 예시를 더욱 명확히 하고자 한다. Alice (A)와 Star Trek(ST)사이의 상호작용을 target y (여기서는 등급을 말함)를 예상함으로써  예측하길 원한다는 것을 가정해보자. 명백하게도, X에 대한 경우는 없기에, 학습데이터 속에 변수 xA와  x ST는 모두 0이 아니기 때문에, 직접 예측은 ‘상호작용 없음' 이라고 유도할 것이다. 그러나, factorized 된 상호작용 파라미터 가 있기에, 이번 경우에도 상호작용을 예상할 수 있다. 먼저, Bob 과 Charlie 는 유사 벡터 를 가지는데, 그 이유는 예측할 등급에 대해,  Star wars와 유사한 상호작용을 했기 때문이다. 즉, 는 와 비슷하다. Alice는 Charlie () 와 다른 factor vector를 갖는데, 그녀는 Titanic 과 Star Wars에 대해 다른 상호작용을 했기 때문이다. 그 다음, Star Trek의 factor vector는 Star Wars와 비슷한 경향성을 보이는데, Bob이 두 영화에 대해 예측 y라는 유사 상호작용을 했기 때문이다. 종합하면, Alice 와 Star Trek의 factor vector 에 대한 내적 (즉, interaction)은 Alice - Star Wars와 유사할 것이다 - 이는 직관적이다.

4) Computation 계산
      다음, 우리는 계산적 관점에서 FM에 대해 살펴볼 것이다. 방정식 (1)의 쭉 뻗은 계산의 복합성은 O() 라고 하는데, 모든 쌍으로 묶인 상호 작용이 계산되어야 하기 때문이다. 하지만 이는 선형 런타임으로 떨어뜨려 변형할 수 있다. 
Lemma 3.1 : factorization machine 의 모델 방정식은 선형 시간 O안에 계산된다.
Proof : 쌍으로 묶은 상호작용의 인자화 때문에, 두 변수에 직접 의존하는 모델 파라미터는 없다. (예를 들어, 인덱스 (i, j_ 의 파라미터) 그래서, 쌍으로 묶인 상호작용은 아래와 같이 다시 한번 공식화 된다.

  이 방정식은 k 와 n 사이의 선형 복잡성만 가지고 있다 , 그래서, 이것은 O 수준의 계산만 가지고 있다. 더욱이, 희소성을 띄는 X 안의 대부분의 원소들은 0으로 구성된다. 그렇기에, 총 합산은 0이 아닌 원소들만 계산하면 된다. 희소 어플리케이션 속에서, factorization machine의 계산은 O( ) 에 불과하다. 예를 들어,  MF 접근과 같은 특정 추천 시스템에서는 =2 이다. 

### B. Factorization Machines as Predictors

FM은 다양한 예측 과제에 적용될 수 있다. 아래 중에서 가능하다
●	회귀 (regression) : 목적 함수 y(x)는 예측기로써 바로 사용될 수 있고, 최적화 기준은 최소 제곱 오차 D를 최소화 하는 것이다.
●	이진 분류 (Binary classification) : 목적 함수 y(x) 의 사인이 사용되고, 파라미터는 hinge 손실이니 logit 손실로 최적화 한다.
●	등급화 (Ranking) : 벡터 x 는 목적함수 y(X)의 점수로 정렬되고, 최적화는 쌍 분류 손실 ([5]과 유사) 인스턴스 벡터의 짝으로 진행된다.
모든 경우에서, L2와 같은 정규화 용어는 오버 피팅을 예방하기 위해 최적화 목적으로 추가된다. 

### C. Learning Factorization Machines

보여준 것과 같이, FM는 선형 시간 동안에 계산될 수 있는 폐쇄 방정식이다. 그러므로, FM의 모델 파라미터 는 gradient descent method에 의해 효과적으로 학습될 수 있다. 예를 들어 확률적 경사 하강법 (SGD)과 같이 말이다. 다양한 손실의 경우, 그 중 제곱, 로짓, 힌지 손실이 있다. FM 모델의 그라디언트는 아래와 같다. 
합산 의 값은 i 에 독립적이기 때문에, 목적 함수 y(x)를 계산 할 때 미리 계산될 수 있다. 일반적으로, 각 경사도는 상수 시간 O(1) 동안 계산될 수 있다. 그리고, 희소성 하에서, 각 경우 (X, y)에 파라미터 업데이트는 또는 동안에 완료된다.
우리는 일반적인 구현체 를 제공한다. 이는 SGD를 사용하고, 요소 별 쌍방향 손실을 제공한다. 

### D. d-way Factorization Machine

지금까지 설명한 2방향 FM은 d-way FM으로 쉽게 일반화 될 수 있다. 

l 번째 상호작의 경우, 상호작용 파라미터는 PARAFAC 모델 [1]에 의해 인자화되고, 그 모델 파라미터는 아래와 같다. 

방정식 (5) 의 직관적인 복잡성은 이다. 하지만, lemma 3.1에서와 같은 동일한 인자와 함께, 이는 선형 시간이 계산될 수 있다는 것을 보여준다. 

### E. Summary 요약

FM 은 전체 파라미터화 한 것 대신,  factorized 상호작용을 사용한 피쳐 벡터 x 내 값들 사이에서 모든 가능한 상호작용을 모델화 한다. 이는 두 가지 이점이 있다.
1)	값들 사이의 상호작용은 높은 희소석 하에서도 예측될 수 있다. 특히, 관측되지 않은 상호작용의 일반화도 가능하다.
2)	파라미터 수 뿐만 아니라, 예측과 학습을 위한 시간은 선형이다. 이는 실행할 수 있는 SGD를 사용한 즉각적인 최적화를 가능하게 하고, 다양한 손실 함수 중에서 최적화 할 수 있도록 한다.
이번 논문을 되짚어 보자면, 우리는 factorization machines 과 support vector machines 간의 관계를 매트릭스 뿐만 아니라 텐서, 특화된 factorization 모델로 보여준다.


## 4. FMs VS. SVMs


### A.	SVM 모델

  SVM [6]의 모델 방정식은 변형된 인풋 x 값과 모델 파라미터  사이의 내적을 표현한다. 이것은 더 복잡한 공간   속에서 피쳐 스페이스 과 매핑된다. 매핑된 은 아래 커널과 관련이 있다.

  아래에서, 우리는 SVM 의 초기 형태를 분석함으로써, FM과 SVM간의 관계를 이야기 한다. 
  1)	선형 커널 : 가장 단순한 커널은 선형 커널이다 : , 이것은 과 매핑된다. 그래서, 선형 SVM 모델 방정식은 아래와 같이 다시 한번 표현할 수 있다. 선형 SVM( 방정식 (7)) 은 d = 1인 FM (방정식(5)) 와 동일하다.
 
  2)	다항식 커널 : 다항식 커널은 SVM가 변수 간 더 높은 상호작용을 모델링 하도록 유도한다. 이는 로 정의되는데, d = 2일때, 아래 매핑과 일치한다. 

그리고, 다항식 SVM 의 모델 방정식은 아래와 같이 다시 작성 가능하다.

이 모델의 파라미터는 아래와 같다. 

방정식 (9) 의 다항 SVM과 FM (방정식 (1))를 비교하면, 두 모델은 모두 상호작용 d = 2에까지 중첩된 것을 알 수 있다. SVM 와 FM의 주요 차이점은, 매개변수화 이다 : SVM의 모든 상호작용 파라미터 는 완전히 독립적이다. 예를 들면 들을 말한다. 대조적으로, FM의 상호작용 파라미터들은 인자화 되었기 때문에, 오버랩 되고, 파라미터를 공유한 것(여기에서는 vi) 처럼  와은 서로에게 의존한다. 

### B. Parameter Estimation Under Sparsity 

아래에서, 우리는 왜 선형 및 다항식 SVM가 매우 희소성 높은 문제에서 실패하는 지 보여줄 것이다. 우리는 유저와 아이템 지표 변수를 통해 협업 필터링 예제를 보여준다. (figure 1 에 나온 첫번째 두 그룹 , 파랑과 빨강을 보라)피쳐 벡터들은 매우 희소해서, 두가지 요소들만 0이 없다. (활성 유저는 u로, 활성 아이템은 i 로 기재)
  1)	선형 SVM :  data X 와 같은 종류에서는 선형 SVM (방정식 (7))은 아래와 같다.
  
  그 이유는, 단지  일때 이기 때문이다. 가장 일반적인 협업 필터링 모델 중 하나와, 이 모델의 일치성은 매우 간단하다, 파라미터들은 희소성 하에서도 잘 예측한다. 그러나 경험적 예측 수준은 전형적으로 낮다 ( figure 2를 통해 확인 가능) 

  2)	다항식 SVM : 다항식 커널을 통해, SVM은 고차원 상호작용이 가능하다 
( 유저 및 아이템 사이) m(X)=2 일 때 희소 케이스에서, SVM 를 위한 모델 방정식은 아래와 같다.

먼저, 와 는 같은 것을 표현한다 - 즉, 중 하나를 제외한다. 이제 모델 방정식은 선형의 경우와 비슷하지만, 추가적인 유저 - 아이템간 상호작용 를 갖는다. 전형적인 협업 필터링 문제 (CF)에서, 각 상호 작용 파라미터 경우, 학습 데이터 속의 최대 한개의  (u, i)가 있으나, 테스트 데이터 속의 (u’, i’)는 학습 데이터 속에서 발견하기 어렵다. figure 1의 예시에서는, (Alice, Titanic) 단 한개의 관찰치만 있었으며, (Alice, Star Trek) 이 둘의 상호 작용은 없었다. 이는 모든 경우 (u, i)에 대해 상호작용 파라미터 들의 최대 마진 솔루션이 0이라는 것을 의미한다.  그리고, 다항식 SVM은 테스트 예시를 예측하기 위해 어떠한 2방향 상호작용을 적용하지 않았다. 그래서, 다항식 SVM은 유저와 아이템 편차에만 의존하고, 선형 SVM에 비해 더 나은 예측값을 제공할 수 없다.
SVM경우, 고차원 상호작용을 예측하는 것은 협업 필터링 만의 이슈가 아니고, 데이터가 크게 희소한 어떤 경우에서든 발생한다. 하지만 , 쌍을 이룬 상호작용 (i, j)의 파라미터 의 신뢰할 만한 예측을 위해, 일때 인 경우가 ‘충분' 하다. 이거나 가 되자마자, X 는 파라미터  예측에 사용될 수 없다. 요약하면, 만약 데이터가 너무 희소할 경우, 즉, (i, j) 경우가 너무 적거나 없을 경우, SVM 는 실패한다. 

### C. Summary

  1)	SVM 의 밀도있는 매개변수는 상호작용을 위한 즉각적인 관찰을 요구한다. 이는 희소한 셋팅에서는 거의 주어지지 않는다. FM의 파라미터는 희소성 하에서도 충분히 잘 예측된다 (section 3 - A3)
  2)	FM은 초기에 즉시 학습할 수 있다. 비선형 SVM는 주로 두가지를 학습한다.
  3)	FM 모델 방정식은 학습 데이터에 독립적이다. SVM 활용한 예측은 학습데이터의 한 부분에 의존한다 (support vectors)


## 5. FM vs. OTHER FACTORIZATION MODELS 


범주 형 변수에 대한 m-ary 관계로 맺어진 표준 모델부터 특별한 데이터와 과제를 위해 특화된 모델(SVD++, PITF, FPMC 등)에 이르는 다양한 인수 분해 모델이 있다. (MF, PARAFAC 등). 우리는 FM이 이러한 모델 다수를 적당한 인풋 데이터(feature vector X등) 를 사용함으로 써 흉내낼 수 있음을 보여준다. 

### A.	Matrix and Tensor Factorization 
행렬 인수화 (MF)는 가장 연구되는 factorization model 중 하나이다. ([7],[8],[2]) 이는 두 범주형 변수 간의 관계를 인수화 한다. 그리고 범주형 변수들을 다루기 위한 기본 접근은 각 U와 I 수준에 대한 이진 지표 변수로 정의된다. ( fig 1 파란색, 빨간색 그룹 참고)

     Feature vector X를 사용하는 FM은 matrix factorization 모델과 동일한데, 그 이유는 xj 는 u와 i 가 0이 아니기 때문에, 다른 편향 및 상호작용은 떨어진다.

같은 인자를 활용하여, 2개 이상의 범주형 변수를 활용한 문제를 해결할 수 있다. FM은 중첩된 병렬 요인 분석 모델을 포함한다 (PARAFAC) [1]

### B. SVD ++
회귀와 같은 비율 예측 과제에서, Koren은 matrix factorization model이 SVD ++ 모델로 개선한다. [2] FM은 이 모델을 인풋 데이터 x (figure 1의 3번째 그룹과 같음)를 사용함으로써 모델을 묘사한다. 

는 유저 u가 등급매긴 모든 영화 세트를 의미한다.FM (d = 2) 는 이 데이터를 사용하여 아래와 같이 행동한다. 

첫번째 파트는 SVD +++ 모델과 완전히 같다. 그러나, FM 는 추가적인 영화 의 기본적인 효과 뿐만 아니라,  유저 U 와 영화 상호작용, 그리고 안의 영화 간 상호작용을 포함한다.

### C. PITF for Tag Recommendation
태그 예측 문제는 주어진 유저와 아이템 조합의 랭킹 태그로 정의한다. 3개 범주형 도메인이 포함된다는 의미를 가진다 : 유저 U, 아이템 I, 태그 T. 태그 추천에 대한 ECML / PKDD Discovery Challenge 에서, factorizing 쌍 상호작용에 기반하여 (PITF) 는 최고 점수를 얻을 수 있다 [3] . 우리는 어떻게 FM이 이 모델을 묘사할 수 있는지 보여줄 것이다. 활성 유저 u, 아이템 i, 태그 t 를 활용한 이진 지표 변수 Factorization machine 는 아래 모델 결과를 낸다. 

같은 유저 / 아이템 조합을 가진, 두 태그 t A, tB 사이의 랭킹을 위해 이 모델이 사용됨으로써, 최적화와 예측은 각 경우와  사이에서 항상 차이점을 보인다. 그러므로, 쌍을 이룬 랭킹 최적화를 위한 FM모델은 아래 방정식을 갖는다. 


이제, 기존 PITF모델[3]과, 이진 변수 (방정식 14) 를 갖는 FM모델은 거의 동일하다. 단 하나의 차이점은, (i) FM 모델은 편향 용어 t에 대한  wt를 가지고, (ii) (u,t) - 와 (i,t) 상호작용 사이의 태그 (vt)를 위한 factorization parameter는 FM 모델에 공유되지만, 기존 PITF 모델에는 사용되지 않는다. 게다가, 이 이론적인 분석에서, figure 3 은 경험적으로 각 모델은 비교할만한 예측 퀄리티를 보여주고 있다
.

### D. Factorized Personalized Markov Chains (FPMC)
FPMC 모델 [4] 는 온라인 샵에서 유저 u의 최근 구매 (t-1 시간) 에 기반하여 상품 등급매기고자 한다. feature generation 에 의하자면, 다시, factorizaton machine (d = 2) 행동은 아래와 유사하다. 


는 유저 u가 시간 t 에 구매한 모든 상품의 세트인 ‘basket’ 을 의미하며 (구체적인 부분은 [4] 를 보라) , 그리고

태그 추천과 같이, 이 모델은 랭킹 (랭킹 아이템 i)을 위해 사용되고 최적화 된다. 그렇기에, 와  두 사이의 점수차이는 예측과 최적화 기준 [4] 로 사용된다. 그러므로, i에 의존하지 않는 모든 추가 term들과 FM 모델 방정식에 관한 식은 아래와 같다. 

이제 기존 FPMC 모델 [4] 와 FM 모델은 거의 동일하며, 차이점은 추가적인 편향 wi이며, 간의 상호작용 속의 factorization parameters 는 함께 활용된다. 

### E. Summary 

  1)	PARAFAC 또는 MF 와 같은 기본 factorization 모델은 factorization machine 과 같은 일반적인 예측 모델이 아니다. 그 대신, feature vector가 m 개로 나누어지고, 각 파트는 완전히 한개의 요소인 1로, 그 나머지는 0으로 채워지는 것이 필요하다. 
  2)	단 하나의 과제를 위해 설계된 특화된 factorization 모델에 대한 많은 제안들이 있다. 우리는 factorization machine들이 다수의 성공적인 factorization 모델 (MF, PARAFAC, SVD+, PITF, FPMC) 를 FM이 쉽게 실용적으로 적용될 수 있도록 하는 feature extraction 을 통해 따라할 수 있음을 보여준다.


## 6. CONCLUSION AND FUTURE WORK


  이번 논문에서, 우리는  factorization machine들을 가져와서,factorization 모델의 이점과 함께 SVM의 일반성을 소개하였다. SVM 과 반대로, (1) FM 은 높은 희소성 하에서도 파라미터 예측이 가능하고 (2) 모델 방정식은 선형적이며, 모델 파라미터에만 의존하기 때문에 (3) 초기에 바로 최적화 가능하다. FM의 표현력은 다항식 SVM중의 하나와 견줄만 하다. tensor factorization model과 비교해보면, FM은 일반적인 예측기이며, 다른 실제 가치 벡터들을 다룰 수 있다. 더욱이, 그 인풋 피쳐 백터 속의 적절한 지표들을 단순화 하여, FM은 동일하거나 매우 유사하게 많은 ‘특화된 최신 모델'의 성능을 보여줄 수 있다. 

