---
layout: post
title: "A recipe for training neural networks 신경망 학습 가이드"
author: "Seriouscoding"
---

오역이 있을시 말씀주시면 수정하겠습니다.

**원제:  recipe for training neural networks** [post url](http://karpathy.github.io/2019/04/25/recipe/)

**저자: Andrej Karpathy**


______________________________________________________________


몇주전 저는 "the most common neural net mistakes" 라는 이름의 트윗을 작성한 적이 있는데, 신경망 학습과 관련된 몇 가지 기본적인 리스트였습니다. 제가 예상했던 것 보다 더 많은 관심을 가졌습니다. 수 많은 사람들이 "컨볼루션 레이어가 어떻게 작동하는지 설명합니다" 라는 것과 "우리의 convnet 이 최신의 우수한 성과를 거뒀습니다" 라는 말 사이에 큰 차이를 느낀다는 것을 개인적으로 경험하였습니다. 

그래서 저는 제 트윗을 이 dusty허접한 블로그에 좀더 길게 적으면 재미있겠다 생각했습니다. 그러나, 더 일반적인 에러들의 enmeration 이나 그것들을 삭제하는 것 대신에, 저는 더 깊게 파고 들어서, 어떻게 연구자들이 발생시킬만한 에러를 피할 수 있는지에 대해 이야기를 하기로 했습니다. (혹은 재빨리 제거할 수 있는지에 대해 말입니다.) 이렇게 하는 트릭은 일련의 프로세스이며, 빈번하게 문서화 되지는 않은 것으로 알고 있습니다. 이러한 동기부여를 준 두 가지 중요한 관찰에 대해 시작해보겠습니다. 

1) Neural net training is a leaky abstraction 신경망 학습은 leaky 한 추론입니다. 

신경망 훈련을 시작하는 것은 아주 쉽습니다. 수많은 라이브러리와 프레임워크는 데이터 문제를 해결하는 30줄짜리 놀라운 스니펫으로 표시하는 데 자신감을 갖고 있어서, 이 항목이 plug and play(쉽게 빼고 낄 수 있는) 거짓 인상을 줍니다. 일반적으로, 다음과 같습니다. 

```python
>>> your_data = # plug your awesome dataset here
>>> model = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)
```

이러한 라이브러리와 예시는 일반 소프트웨어에 익숙한 여러분의 뇌의 한 부분을 활성화 시킵니다. - 깔끔한 API와 추상화를 진행하는 그 곳은 가끔 도달할만 합니다. Request 라이브러리를 보여드리자면:

```python
>>> r = requests.get('https://api.github.com/user', auth=('user', 'pass'))
>>> r.status_code
200
```

멋집니다. 여러분과 같은 의욕있는 개발자들은 쿼리문, url, GET/POST requests, HTTP connection 등, 그리고 몇 줄의 코드 너머 크게 숨겨져있는 복잡성을 이해하는 것의 부담을 가지고 있습니다. 이것이 우리가 익숙한 것이며 예상한 것입니다. 하지만 안타깝게도 신경망은 그렇지 않습니다. 이들은 ImageNet 분류 모델 훈련에서 약간 벗어난 "off-the-shelf(기성품)" 이 아닙니다. 제가 이 부분에 대해 제 포스트 "[Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)" 에서도 역전파를 'leaky abstraction' 이라고 언급하였습니다. 하지만 상황은 안타깝게도 더 어려워졌습니다. 역전파 + SGD 는 여러분의 네트워크에서 마법같이 작동하지 않습니다. Batch norm 은 마법처럼 빠르게 converge 되지 않을 것입니다. RNN은 여러분이 텍스트를 'plug in(꽂아 넣기)' 마법처럼 두지 않을 것입니다. 그리고 문제를 RL(강화학습)로 공식화 할 수 있다고 해서 반드시 그래야 하는 것은 아닙니다. 만약 여러분이 작동방식을 이해하지 않고 계속 사용한다면, 이는 실패할 가능성이 높습니다. 그리고 또...

2) Neural net training fails silently 신경망 학습은 조용히 실패하고 있습니다.

여러분은 코드가 break되거나 misconfigure 될 경우, 몇몇 예외사항에 대한 정보를 얻을 수 있습니다. 문자열 입력을 기대한 곳에 정수를 넣거나 할 때 말입니다. 이 함수는 3개의 인자만 넣기를 기대합니다. 이 import 는 실패합니다. 어떤 key는 존재하지 않습니다. 2개의 리스트 길이가 같지 않습니다. 추가적으로, 일련의 함수를 위한 unit test 도 가능합니다. 

이는 신경망 학습을 시작할 때 벌어지는 것입니다. 모든 것들은 인위적으로 완벽합니다, 그러나 몇몇은 적절하게 놓여있지 않고 이는 쉽게 알아채기 어렵습니다. "가능성 있는 에러 표면" 은 커다랗고 논리적(인위적인 것의 반대) 이며, 부분 테스트를 진행하기에는 매우 조악합니다. 예를 들어, 여러분이  데이터 증강(data augmentation) 하는 동안 왼-오 뒤집어져있을 때, 여러분의 라벨을 뒤집는 것(filp)을 깜박하였다고 가정해봅시다. 여러분의 신경망은 여전히(놀랍게도) 꽤 잘 동작할 것인데, 그 이유는 여러분의 신경망이 내부적으로 뒤집어진 이미지를 감지하고, 그것의 예측값을 왼-오 뒤집어 결과를 내기 때문입니다. 또는, 자동회귀 모델이 우연하게 off-by-one bug 로 인해 예측하려는 것을 입력값으로 사용할 수도 있습니다. 또는  여러분의 loss 를 수집하는 것이 아니라 gradients를 수집하게 되어 학습하는 동안 이상치 예제가 무시될 수도 있습니다. 또는 여러분의 가중치가 pretrained checkpoint에서 얻은 가중치가 초기화 되었는데 원래 데이터의 평균을 사용할 수 없을 수도 있습니다. 또는 여러분은 정규화 강도, 학습률, decay rate, 모델크기 등을 셋팅하는데 어려움을 겪을수도 있습니다. 그러므로, 여러분의 잘못 구성된 신경망은 운이 좋을때만 그 예외를 뱉어냅니다. 대부분의 경우, 이는 조용하게 잘못 학습됩니다. 

그 결과, (그리고 더 강조하기가 정말 어려운데) "fast and furious(빠르고 분노에 가득찰 정도로 화끈한?) " 신경망 학습 접근은 작동하지 않으며 여러분에게 고통만 안겨줄 것입니다. 고통은 완벽하게 자잘 작동하는 신경망을 얻기 위한 자연적인 부분( 자연적인 수순) 입니다. 그러나, 철처하고 방어적이며, 편집증적이고 기본적으로 가능한 모든것들을 시각화 하려는 것에 집작하게 되면 완화할 수 있습니다. 제 경험에 따르면, 딥러닝의 성공과 가장 밀접한 관련이 있는 자질은 인내심과 세부 사항에 대한 관심입니다. 
